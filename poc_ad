{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7f23b4fe4cec4e058187f3871d8355b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d83f31c58f554c32bc7d73f8876584d4",
              "IPY_MODEL_2319bf23767242bebf5ab8e48b67808c",
              "IPY_MODEL_ddde5ea543fd4e76bdb7beb550cd6515"
            ],
            "layout": "IPY_MODEL_553b6dd48ba44a45a60793cb851e8962"
          }
        },
        "d83f31c58f554c32bc7d73f8876584d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f53b2bb6d8fa41e097bb63263985a06f",
            "placeholder": "​",
            "style": "IPY_MODEL_a0bc90ccd21f4094a46d0c84fbf85374",
            "value": "  0%"
          }
        },
        "2319bf23767242bebf5ab8e48b67808c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0181352df4a34ec7a2e0151d0adb1d11",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b23bd7b2bfb2467da45ced9786263a7f",
            "value": 0
          }
        },
        "ddde5ea543fd4e76bdb7beb550cd6515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ef9aea76e52400a8a500119cea99626",
            "placeholder": "​",
            "style": "IPY_MODEL_2531d3d9a4e54ceba090d895eccffe01",
            "value": " 0/50 [00:00&lt;?, ?it/s]"
          }
        },
        "553b6dd48ba44a45a60793cb851e8962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f53b2bb6d8fa41e097bb63263985a06f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0bc90ccd21f4094a46d0c84fbf85374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0181352df4a34ec7a2e0151d0adb1d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b23bd7b2bfb2467da45ced9786263a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ef9aea76e52400a8a500119cea99626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2531d3d9a4e54ceba090d895eccffe01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "Mg5kYH3vGQhx",
        "outputId": "88cd9c95-ddf5-41a8-c546-86050fa95384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated synthetic dataset with 1000 base records and 5% anomalies.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Customer ID     Customer Name  \\\n",
              "0          333         Ellen Lee   \n",
              "1          353    Javier Winters   \n",
              "2          943  Bridget Phillips   \n",
              "3            8    Kristi Vasquez   \n",
              "4          569   Michael Mueller   \n",
              "\n",
              "                                             Address  Country  \\\n",
              "0  0879 Michael Plaza Apt. 070, New Williamside, ...   Canada   \n",
              "1    4488 Hansen Junctions, Port Lisamouth, TN 62442  Germany   \n",
              "2                   PSC 0501, Box 9268, APO AP 87867      USA   \n",
              "3  920 Ortiz Key Suite 840, Katherineburgh, AL 12168   Canada   \n",
              "4  86839 Shannon Light Suite 556, Lake Tammy, PW ...  Germany   \n",
              "\n",
              "       Transaction Timestamp  Transaction Amount Transaction Location  \\\n",
              "0 2024-06-11 04:29:04.071077             1297.33               Canada   \n",
              "1 2025-04-24 20:49:04.528329             3533.98              Germany   \n",
              "2 2024-10-29 17:29:04.648805             2503.92                  USA   \n",
              "3 2025-02-14 09:15:04.434398             1263.96               Canada   \n",
              "4 2024-09-05 01:09:04.469735             4883.44              Germany   \n",
              "\n",
              "   Payer Customer ID  Payee Customer ID Risk Profile  Account Number  \\\n",
              "0                333                755          Low       100000333   \n",
              "1                353                 57          Low       100000353   \n",
              "2                943                142          Low       100000943   \n",
              "3                  8                542          Low       100000008   \n",
              "4                569                861          Low       100000569   \n",
              "\n",
              "    Account Type Channel Account Status  Base Balance Currency  \n",
              "0        Savings  Online       Inactive       2424.96      EUR  \n",
              "1        Savings     ATM         Closed      66210.67      INR  \n",
              "2        Savings  Branch         Closed      65490.62      GBP  \n",
              "3        Savings     ATM         Active      86751.44      INR  \n",
              "4  Fixed Deposit     ATM         Active      90511.71      GBP  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a692b0ee-95d4-4218-9a70-a92458f6b492\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>Address</th>\n",
              "      <th>Country</th>\n",
              "      <th>Transaction Timestamp</th>\n",
              "      <th>Transaction Amount</th>\n",
              "      <th>Transaction Location</th>\n",
              "      <th>Payer Customer ID</th>\n",
              "      <th>Payee Customer ID</th>\n",
              "      <th>Risk Profile</th>\n",
              "      <th>Account Number</th>\n",
              "      <th>Account Type</th>\n",
              "      <th>Channel</th>\n",
              "      <th>Account Status</th>\n",
              "      <th>Base Balance</th>\n",
              "      <th>Currency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>333</td>\n",
              "      <td>Ellen Lee</td>\n",
              "      <td>0879 Michael Plaza Apt. 070, New Williamside, ...</td>\n",
              "      <td>Canada</td>\n",
              "      <td>2024-06-11 04:29:04.071077</td>\n",
              "      <td>1297.33</td>\n",
              "      <td>Canada</td>\n",
              "      <td>333</td>\n",
              "      <td>755</td>\n",
              "      <td>Low</td>\n",
              "      <td>100000333</td>\n",
              "      <td>Savings</td>\n",
              "      <td>Online</td>\n",
              "      <td>Inactive</td>\n",
              "      <td>2424.96</td>\n",
              "      <td>EUR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>353</td>\n",
              "      <td>Javier Winters</td>\n",
              "      <td>4488 Hansen Junctions, Port Lisamouth, TN 62442</td>\n",
              "      <td>Germany</td>\n",
              "      <td>2025-04-24 20:49:04.528329</td>\n",
              "      <td>3533.98</td>\n",
              "      <td>Germany</td>\n",
              "      <td>353</td>\n",
              "      <td>57</td>\n",
              "      <td>Low</td>\n",
              "      <td>100000353</td>\n",
              "      <td>Savings</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Closed</td>\n",
              "      <td>66210.67</td>\n",
              "      <td>INR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>943</td>\n",
              "      <td>Bridget Phillips</td>\n",
              "      <td>PSC 0501, Box 9268, APO AP 87867</td>\n",
              "      <td>USA</td>\n",
              "      <td>2024-10-29 17:29:04.648805</td>\n",
              "      <td>2503.92</td>\n",
              "      <td>USA</td>\n",
              "      <td>943</td>\n",
              "      <td>142</td>\n",
              "      <td>Low</td>\n",
              "      <td>100000943</td>\n",
              "      <td>Savings</td>\n",
              "      <td>Branch</td>\n",
              "      <td>Closed</td>\n",
              "      <td>65490.62</td>\n",
              "      <td>GBP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>Kristi Vasquez</td>\n",
              "      <td>920 Ortiz Key Suite 840, Katherineburgh, AL 12168</td>\n",
              "      <td>Canada</td>\n",
              "      <td>2025-02-14 09:15:04.434398</td>\n",
              "      <td>1263.96</td>\n",
              "      <td>Canada</td>\n",
              "      <td>8</td>\n",
              "      <td>542</td>\n",
              "      <td>Low</td>\n",
              "      <td>100000008</td>\n",
              "      <td>Savings</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Active</td>\n",
              "      <td>86751.44</td>\n",
              "      <td>INR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>569</td>\n",
              "      <td>Michael Mueller</td>\n",
              "      <td>86839 Shannon Light Suite 556, Lake Tammy, PW ...</td>\n",
              "      <td>Germany</td>\n",
              "      <td>2024-09-05 01:09:04.469735</td>\n",
              "      <td>4883.44</td>\n",
              "      <td>Germany</td>\n",
              "      <td>569</td>\n",
              "      <td>861</td>\n",
              "      <td>Low</td>\n",
              "      <td>100000569</td>\n",
              "      <td>Fixed Deposit</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Active</td>\n",
              "      <td>90511.71</td>\n",
              "      <td>GBP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a692b0ee-95d4-4218-9a70-a92458f6b492')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a692b0ee-95d4-4218-9a70-a92458f6b492 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a692b0ee-95d4-4218-9a70-a92458f6b492');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d9dcf032-5e24-44a4-a742-04f5f3cbc4c4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d9dcf032-5e24-44a4-a742-04f5f3cbc4c4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d9dcf032-5e24-44a4-a742-04f5f3cbc4c4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1050,\n  \"fields\": [\n    {\n      \"column\": \"Customer ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 291,\n        \"min\": 1,\n        \"max\": 996,\n        \"num_unique_values\": 630,\n        \"samples\": [\n          68,\n          317,\n          649\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Customer Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 628,\n        \"samples\": [\n          \"Karen Adkins\",\n          \"Matthew Maldonado\",\n          \"Javier Miles\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Address\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 630,\n        \"samples\": [\n          \"0499 Cuevas Village, Port Kenneth, OH 85551\",\n          \"1448 Gill Falls Apt. 277, Smithborough, NJ 58700\",\n          \"85435 Kimberly Stravenue Apt. 122, Taylormouth, OK 11082\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Germany\",\n          \"UK\",\n          \"USA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Transaction Timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-06-05 12:50:04.448407\",\n        \"max\": \"2025-06-06 04:26:04.227440\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"2024-12-26 15:32:04.155074\",\n          \"2025-04-07 00:36:04.344046\",\n          \"2025-03-12 22:28:04.051470\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Transaction Amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 128510.09300258885,\n        \"min\": 11.8,\n        \"max\": 4033375.0,\n        \"num_unique_values\": 1046,\n        \"samples\": [\n          1667.87,\n          3207.67,\n          3641.73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Transaction Location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Canada\",\n          \"Germany\",\n          \"UK\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Payer Customer ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 291,\n        \"min\": 1,\n        \"max\": 996,\n        \"num_unique_values\": 630,\n        \"samples\": [\n          68,\n          317,\n          649\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Payee Customer ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 283,\n        \"min\": 1,\n        \"max\": 1000,\n        \"num_unique_values\": 641,\n        \"samples\": [\n          628,\n          502,\n          387\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Risk Profile\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Low\",\n          \"High\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Account Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 291,\n        \"min\": 100000001,\n        \"max\": 100000996,\n        \"num_unique_values\": 630,\n        \"samples\": [\n          100000068,\n          100000317,\n          100000649\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Account Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Savings\",\n          \"Fixed Deposit\",\n          \"Current\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Channel\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"ATM\",\n          \"Mobile\",\n          \"Online\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Account Status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Inactive\",\n          \"Closed\",\n          \"Active\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Base Balance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28767.639039441063,\n        \"min\": 1501.1,\n        \"max\": 99667.05,\n        \"num_unique_values\": 630,\n        \"samples\": [\n          80417.5,\n          41486.34,\n          79127.95\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Currency\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"INR\",\n          \"USD\",\n          \"GBP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "### Generate Synthetic Banking Transaction Dataset with 5% Anomalies\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from faker import Faker\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "fake = Faker()\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "n_records = 1000\n",
        "n_anomalies = int(0.05 * n_records)\n",
        "\n",
        "# Define base lists\n",
        "countries = ['USA', 'UK', 'Canada', 'India', 'Germany']\n",
        "account_types = ['Savings', 'Current', 'Fixed Deposit']\n",
        "channels = ['Online', 'Mobile', 'ATM', 'Branch']\n",
        "account_statuses = ['Active', 'Inactive', 'Closed']\n",
        "currencies = ['USD', 'EUR', 'INR', 'GBP', 'CAD']\n",
        "risk_profiles = ['Low', 'Medium', 'High']\n",
        "\n",
        "# Generate base customer info\n",
        "customers = []\n",
        "for i in range(1, n_records + 1):\n",
        "    customers.append({\n",
        "        'Customer ID': i,\n",
        "        'Customer Name': fake.name(),\n",
        "        'Address': fake.address().replace(\"\\n\", \", \"),\n",
        "        'Country': random.choice(countries),\n",
        "        'Account Number': 100000000 + i,\n",
        "        'Account Type': random.choice(account_types),\n",
        "        'Account Status': random.choice(account_statuses),\n",
        "        'Base Balance': round(np.random.uniform(1000, 100000), 2),\n",
        "        'Currency': random.choice(currencies),\n",
        "        'Risk Profile': random.choice(risk_profiles)\n",
        "    })\n",
        "\n",
        "customer_df = pd.DataFrame(customers)\n",
        "\n",
        "# Generate transactions\n",
        "transactions = []\n",
        "for i in range(n_records):\n",
        "    customer = customer_df.sample(1).iloc[0]\n",
        "    payer_id = customer['Customer ID']\n",
        "    payee_id = random.choice(customer_df['Customer ID'].values)\n",
        "    timestamp = datetime.now() - timedelta(days=random.randint(0, 365), hours=random.randint(0, 23), minutes=random.randint(0, 59))\n",
        "    amount = round(np.random.uniform(10, 5000), 2)\n",
        "    txn = {\n",
        "        'Customer ID': payer_id,\n",
        "        'Customer Name': customer['Customer Name'],\n",
        "        'Address': customer['Address'],\n",
        "        'Country': customer['Country'],\n",
        "        'Transaction Timestamp': timestamp,\n",
        "        'Transaction Amount': amount,\n",
        "        'Transaction Location': customer['Country'],\n",
        "        'Payer Customer ID': payer_id,\n",
        "        'Payee Customer ID': payee_id,\n",
        "        'Risk Profile': customer['Risk Profile'],\n",
        "        'Account Number': customer['Account Number'],\n",
        "        'Account Type': customer['Account Type'],\n",
        "        'Channel': random.choice(channels),\n",
        "        'Account Status': customer['Account Status'],\n",
        "        'Base Balance': customer['Base Balance'],\n",
        "        'Currency': customer['Currency']\n",
        "    }\n",
        "    transactions.append(txn)\n",
        "\n",
        "# Introduce 5% anomalies\n",
        "for _ in range(n_anomalies):\n",
        "    record = random.choice(transactions)\n",
        "    anomaly = record.copy()\n",
        "    # Inject anomalies\n",
        "    anomaly['Transaction Amount'] *= 50  # Unusually high amount\n",
        "    anomaly['Transaction Location'] = \"Unknown\"\n",
        "    anomaly['Risk Profile'] = 'High'\n",
        "    transactions.append(anomaly)\n",
        "\n",
        "# Shuffle final dataset\n",
        "df = pd.DataFrame(transactions).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"synthetic_banking_transactions.csv\", index=False)\n",
        "print(\"Generated synthetic dataset with 1000 base records and 5% anomalies.\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load and Preprocess Data\n",
        "df = pd.read_csv('/content/synthetic_banking_transactions.csv')  # Assume numerical data only\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Optional: convert categorical using one-hot\n",
        "df_encoded = pd.get_dummies(df)\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(df_encoded)\n",
        "\n",
        "# Reshape for LSTM: (samples, timesteps, features)\n",
        "X = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# 2. Build LSTM Autoencoder\n",
        "timesteps = X.shape[1]\n",
        "input_dim = X.shape[2]\n",
        "\n",
        "inputs = Input(shape=(timesteps, input_dim))\n",
        "encoded = LSTM(64, activation='relu')(inputs)\n",
        "decoded = RepeatVector(timesteps)(encoded)\n",
        "decoded = LSTM(input_dim, activation='sigmoid', return_sequences=True)(decoded)\n",
        "\n",
        "autoencoder = Model(inputs, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.fit(X, X, epochs=20, batch_size=32, validation_split=0.1, verbose=0)\n",
        "\n",
        "# 3. Calculate Reconstruction Error\n",
        "X_pred = autoencoder.predict(X)\n",
        "recon_error = np.mean(np.power(X - X_pred, 2), axis=(1,2))\n",
        "threshold = np.percentile(recon_error, 95)\n",
        "anomalies = recon_error > threshold\n",
        "\n",
        "# Get anomalous records\n",
        "anomaly_records = df[anomalies]\n",
        "print(\"Anomalous Records:\")\n",
        "print(anomaly_records)\n",
        "\n",
        "# 4. Use SHAP to Explain Anomalies\n",
        "#explainer = shap.DeepExplainer(autoencoder, X[:100])\n",
        "#shap_values = explainer.shap_values(X[anomalies])\n",
        "\n",
        "# Flatten the LSTM input for KernelExplainer\n",
        "X_flat = X.reshape(X.shape[0], -1)\n",
        "background = X_flat[np.random.choice(X_flat.shape[0], 100, replace=False)]\n",
        "\n",
        "import shap\n",
        "\n",
        "# Flatten input: (samples, timesteps × features)\n",
        "X_flat = X.reshape(X.shape[0], -1)\n",
        "\n",
        "# Select background set for SHAP\n",
        "background = X_flat[np.random.choice(X_flat.shape[0], 100, replace=False)]\n",
        "\n",
        "# Define a prediction function on flattened input\n",
        "def predict_fn(x):\n",
        "    x_reshaped = x.reshape(x.shape[0], 1, X.shape[2])\n",
        "    return np.mean(np.square(autoencoder.predict(x_reshaped) - x_reshaped), axis=(1,2)).reshape(-1, 1)\n",
        "\n",
        "# Use KernelExplainer\n",
        "explainer = shap.KernelExplainer(predict_fn, background)\n",
        "\n",
        "# SHAP values for a few anomalies\n",
        "X_anomalies_flat = X_flat[anomalies][:10]  # Only first 10 for speed\n",
        "shap_values = explainer.shap_values(X_anomalies_flat)\n",
        "\n",
        "# Show SHAP summary\n",
        "shap.summary_plot(shap_values, features=pd.DataFrame(X_anomalies_flat, columns=df_encoded.columns))\n",
        "\n",
        "\n",
        "\n",
        "# Display SHAP summary\n",
        "shap.summary_plot(shap_values[0], df_encoded.columns, plot_type='bar')\n",
        "\n",
        "# 5. Derive Simple Rules from SHAP Values\n",
        "mean_shap = np.abs(shap_values[0]).mean(axis=0)\n",
        "top_features = df_encoded.columns[np.argsort(mean_shap)[-5:]].tolist()\n",
        "print(\"Top anomaly-driving features:\", top_features)\n",
        "\n",
        "# Derive rule bounds (e.g., 5th/95th percentile on anomaly records)\n",
        "rules = {}\n",
        "for feat in top_features:\n",
        "    values = anomaly_records[feat]\n",
        "    rules[feat] = {\n",
        "        \"min\": values.quantile(0.05),\n",
        "        \"max\": values.quantile(0.95)\n",
        "    }\n",
        "\n",
        "print(\"Generated rules:\")\n",
        "for feat, bounds in rules.items():\n",
        "    print(f\"If {feat} < {bounds['min']:.2f} or > {bounds['max']:.2f} → Possible anomaly\")\n",
        "\n",
        "# 6. Future Use: Rule-Based Filter\n",
        "def detect_anomaly_with_rules(new_data, rules):\n",
        "    flags = []\n",
        "    for i, row in new_data.iterrows():\n",
        "        is_anomaly = False\n",
        "        for feat, bounds in rules.items():\n",
        "            if row[feat] < bounds[\"min\"] or row[feat] > bounds[\"max\"]:\n",
        "                is_anomaly = True\n",
        "        flags.append(is_anomaly)\n",
        "    return flags\n",
        "\n",
        "# Test rule-based detection\n",
        "future_flags = detect_anomaly_with_rules(df_encoded, rules)\n",
        "print(f\"Anomalies detected by rule engine: {sum(future_flags)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "XgQn2TGfUmac",
        "outputId": "a2b063a5-30a1-4e2c-c396-9acb6991012e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "gradient registry has no entry for: shap_DivNoNan",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-3352f4a2a92d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# 3. Calculate Reconstruction Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtrainable_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: gradient registry has no entry for: shap_DivNoNan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap tensorflow pandas scikit-learn numpy matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r0WVIuuaXlq",
        "outputId": "04cdbec7-ecdd-4e51-a7d2-fe04b7bb1a92"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.13.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure required libraries are installed\n",
        "#!pip install shap tensorflow pandas scikit-learn numpy matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Not used in this code block, but good practice to import if needed later\n",
        "# from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load and Preprocess Data\n",
        "# Load the dataset generated in the previous cell\n",
        "df = pd.read_csv('synthetic_banking_transactions.csv')\n",
        "# Fill NaN values, although the synthetic data might not have them\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Convert categorical features to numerical using one-hot encoding\n",
        "# This is crucial for the autoencoder to process these features\n",
        "df_encoded = pd.get_dummies(df)\n",
        "\n",
        "# Normalize numerical features to be between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(df_encoded)\n",
        "\n",
        "# Reshape data for LSTM input: (samples, timesteps, features)\n",
        "# Here, each transaction is a single timestep\n",
        "X = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# 2. Build LSTM Autoencoder\n",
        "timesteps = X.shape[1] # Should be 1\n",
        "input_dim = X.shape[2] # Number of features after one-hot encoding and scaling\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "inputs = Input(shape=(timesteps, input_dim))\n",
        "# Encoder: LSTM layer to capture sequential patterns (even with timesteps=1)\n",
        "encoded = LSTM(64, activation='relu')(inputs)\n",
        "# Repeat the encoded vector for decoding\n",
        "decoded = RepeatVector(timesteps)(encoded)\n",
        "# Decoder: LSTM layer to reconstruct the input data\n",
        "decoded = LSTM(input_dim, activation='sigmoid', return_sequences=True)(decoded)\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Model(inputs, decoded)\n",
        "# Compile the model with Adam optimizer and mean squared error loss\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the autoencoder first\n",
        "# Use the scaled and reshaped data for training\n",
        "print(\"Training autoencoder...\")\n",
        "# Add error handling or print statements to confirm this step completes\n",
        "try:\n",
        "    autoencoder.fit(X, X, epochs=20, batch_size=32, validation_split=0.1, verbose=0)\n",
        "    print(\"Autoencoder training complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during autoencoder training: {e}\")\n",
        "    # Re-raise the exception if you want it to stop execution\n",
        "    raise\n",
        "\n",
        "# 3. Calculate Reconstruction Error\n",
        "# Predict the reconstructed output for the input data\n",
        "X_pred = autoencoder.predict(X)\n",
        "# Calculate the mean squared error between the original input and the reconstruction\n",
        "# Mean across timesteps (axis=1) and features (axis=2)\n",
        "recon_error = np.mean(np.power(X - X_pred, 2), axis=(1,2))\n",
        "# Define a threshold for anomalies (e.g., 95th percentile of reconstruction errors)\n",
        "threshold = np.percentile(recon_error, 95)\n",
        "# Identify anomalies where the reconstruction error exceeds the threshold\n",
        "anomalies = recon_error > threshold\n",
        "\n",
        "# Get the records from the original dataframe that are identified as anomalous\n",
        "anomaly_records = df[anomalies]\n",
        "print(f\"\\nIdentified {sum(anomalies)} anomalous records (using a threshold of {threshold:.4f}).\")\n",
        "print(\"Anomalous Records (first 5):\")\n",
        "display(anomaly_records.head()) # Use display for better formatting in notebooks\n",
        "\n",
        "# 4. Use SHAP to Explain Anomalies using KernelExplainer\n",
        "# KernelExplainer requires a prediction function that takes 2D input\n",
        "# and a background dataset that is also 2D.\n",
        "# Our model expects 3D input (samples, timesteps, features).\n",
        "# Since timesteps=1, we can reshape the input for the prediction function.\n",
        "\n",
        "# Create a prediction function that reshapes 2D input to 3D for the model\n",
        "def predict_wrapper(x):\n",
        "    # Reshape the 2D input (samples, features) to 3D (samples, 1, features)\n",
        "    x_reshaped = np.reshape(x, (x.shape[0], 1, x.shape[1]))\n",
        "    # Predict using the autoencoder\n",
        "    prediction = autoencoder.predict(x_reshaped)\n",
        "    # The autoencoder predicts the input itself. For anomaly detection using reconstruction error,\n",
        "    # we are interested in explaining *why* the reconstruction error is high.\n",
        "    # SHAP usually explains the output *value*. For anomaly detection, we want to explain\n",
        "    # the *deviation* or the *error*.\n",
        "    # A common approach is to explain the reconstruction error or the distance from the reconstruction.\n",
        "    # Let's try explaining the squared error for simplicity.\n",
        "    # The prediction is (samples, 1, features), input is (samples, 1, features).\n",
        "    # We want to explain the mean squared error for each sample: mean( (x_reshaped - prediction)**2 )\n",
        "    # This output is 1D (samples,). KernelExplainer can explain a 1D output.\n",
        "    return np.mean(np.power(x_reshaped - prediction, 2), axis=(1,2))\n",
        "\n",
        "# Flatten the scaled data for KernelExplainer background and explanation data\n",
        "X_flat = X.reshape(X.shape[0], -1) # Reshape (samples, 1, features) to (samples, features)\n",
        "\n",
        "# Select a background dataset for KernelExplainer (a sample of non-anomalous data)\n",
        "# KernelExplainer performance depends on the size and representativeness of the background data.\n",
        "# A small sample (e.g., 100) is often used for speed, but larger samples improve accuracy.\n",
        "# Let's sample from the non-anomalous data (anomalies == False)\n",
        "background_indices = np.random.choice(np.where(anomalies == False)[0], 100, replace=False)\n",
        "background_data = X_flat[background_indices]\n",
        "\n",
        "# Initialize KernelExplainer with the prediction wrapper and background data\n",
        "print(\"\\nInitializing SHAP KernelExplainer...\")\n",
        "# Note: KernelExplainer can be slow for large datasets/feature counts.\n",
        "# If df_encoded.shape[1] is very large (e.g., thousands), KernelExplainer might take a long time.\n",
        "# The KernelExplainer explains the output of `predict_wrapper`.\n",
        "explainer = shap.KernelExplainer(predict_wrapper, background_data)\n",
        "\n",
        "# Calculate SHAP values for the anomalous instances\n",
        "# Use the flattened data for the anomalies\n",
        "X_anomalies_flat = X_flat[anomalies]\n",
        "\n",
        "# Check if there are anomalies to explain\n",
        "if X_anomalies_flat.shape[0] > 0:\n",
        "    print(f\"Calculating SHAP values for {X_anomalies_flat.shape[0]} anomalous instances using KernelExplainer...\")\n",
        "    # KernelExplainer might print progress bars\n",
        "    shap_values = explainer.shap_values(X_anomalies_flat)\n",
        "\n",
        "    # shap_values from KernelExplainer explaining a 1D output is typically a single array\n",
        "    # with shape (num_anomalies, num_features).\n",
        "    # Display SHAP summary plot\n",
        "    print(\"\\nGenerating SHAP summary plot...\")\n",
        "    # shap.summary_plot expects SHAP values and the data used to compute them.\n",
        "    # Use the flattened anomaly data and feature names from df_encoded\n",
        "    shap.summary_plot(\n",
        "        shap_values, # SHAP values array\n",
        "        X_anomalies_flat, # Data points (2D: samples, features) corresponding to anomalies\n",
        "        feature_names=df_encoded.columns, # Feature names\n",
        "        plot_type='bar', # Use bar plot for overall feature importance\n",
        "        show=False # Prevent immediate plot display, control with plt.show()\n",
        "    )\n",
        "    plt.title('SHAP Feature Importance for High Reconstruction Error (Anomalies)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No anomalies found to explain with SHAP.\")\n",
        "    shap_values = None # Set to None if no anomalies\n",
        "\n",
        "\n",
        "# 5. Derive Simple Rules from SHAP Values\n",
        "rules = {}\n",
        "# Proceed only if SHAP values were successfully calculated and are not None\n",
        "if shap_values is not None and shap_values.shape[0] > 0:\n",
        "    # shap_values shape is (num_anomalies, num_features) for KernelExplainer explaining 1D output\n",
        "    # We want the mean absolute SHAP value for each feature across all anomalies\n",
        "    mean_shap = np.abs(shap_values).mean(axis=0) # Average across anomalies\n",
        "\n",
        "    # Get indices of top features based on mean absolute SHAP values\n",
        "    # Get the top 5 feature indices, ensuring we don't ask for more features than available\n",
        "    num_features_to_select = min(5, len(mean_shap))\n",
        "    top_features_indices = np.argsort(mean_shap)[-num_features_to_select:]\n",
        "\n",
        "    # Get the actual feature names using these indices\n",
        "    top_features = df_encoded.columns[top_features_indices].tolist()\n",
        "    print(f\"\\nTop {num_features_to_select} anomaly-driving features based on SHAP:\", top_features)\n",
        "\n",
        "    # Derive simple rules based on the distribution of these top features in the anomalous records\n",
        "    # Use the original (unscaled, unencoded) anomaly records to find rule bounds\n",
        "    # This is debatable; deriving rules from scaled/encoded data might be more consistent\n",
        "    # with the data the model sees, but rules on original values are more interpretable.\n",
        "    # Let's use the original `anomaly_records` DataFrame for interpretability.\n",
        "    # BUT, the SHAP values were calculated on `df_encoded`, so let's use `df_encoded[anomalies]`\n",
        "    # to derive rules from the feature space SHAP analyzed.\n",
        "    anomaly_records_for_rules = df_encoded[anomalies]\n",
        "\n",
        "    for feat in top_features:\n",
        "        # Check if the feature exists in the anomaly records DataFrame used for rules\n",
        "        if feat in anomaly_records_for_rules.columns:\n",
        "            values = anomaly_records_for_rules[feat]\n",
        "            # Calculate 5th and 95th percentiles as bounds for the rule\n",
        "            # Handle potential cases where the feature column is empty or all NaNs in anomalies\n",
        "            if not values.empty and not values.isnull().all():\n",
        "                 rules[feat] = {\n",
        "                    \"min\": values.quantile(0.05),\n",
        "                    \"max\": values.quantile(0.95)\n",
        "                }\n",
        "            else:\n",
        "                 print(f\"Warning: Feature '{feat}' in anomaly records is empty or contains only NaNs. Skipping rule generation for this feature.\")\n",
        "        else:\n",
        "            # This should ideally not happen if top_features come from df_encoded.columns\n",
        "            print(f\"Warning: Top feature '{feat}' not found in the anomaly records dataframe used for rules.\")\n",
        "\n",
        "    print(\"\\nGenerated rules (based on 5th/95th percentile of encoded anomalies for top SHAP features):\")\n",
        "    if rules:\n",
        "        for feat, bounds in rules.items():\n",
        "            if bounds: # Check if bounds were successfully generated\n",
        "                print(f\"If {feat} < {bounds['min']:.4f} or > {bounds['max']:.4f} → Possible anomaly\")\n",
        "    else:\n",
        "        print(\"No rules generated (either no anomalies or issue with feature processing).\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping rule generation as no SHAP values were calculated or anomalies found.\")\n",
        "\n",
        "\n",
        "# 6. Future Use: Rule-Based Filter\n",
        "# Define a function to detect anomalies using the derived rules\n",
        "def detect_anomaly_with_rules(new_data, rules):\n",
        "    \"\"\"\n",
        "    Applies the generated rules to new data to detect anomalies.\n",
        "\n",
        "    Args:\n",
        "        new_data (pd.DataFrame): The dataframe of new data (must contain the features\n",
        "                                 used in the rules, typically encoded/preprocessed\n",
        "                                 similarly to the data used for rule generation).\n",
        "        rules (dict): A dictionary containing features and their min/max bounds.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of boolean flags indicating if a record is an anomaly (True) or not (False).\n",
        "    \"\"\"\n",
        "    flags = []\n",
        "    # Iterate through each row of the new data\n",
        "    for i, row in new_data.iterrows():\n",
        "        is_anomaly = False\n",
        "        # Check each rule\n",
        "        for feat, bounds in rules.items():\n",
        "             # Ensure the feature exists in the current row and bounds were generated\n",
        "            if feat in row and bounds:\n",
        "                value = row[feat]\n",
        "                # Check if the value violates the defined bounds\n",
        "                # Ensure value is numeric before comparison\n",
        "                if pd.api.types.is_numeric_dtype(type(value)) and not np.isnan(value):\n",
        "                     if value < bounds[\"min\"] or value > bounds[\"max\"]:\n",
        "                        is_anomaly = True # Flag as anomaly if any rule is violated\n",
        "                        # break # Optional: Uncomment to flag based on the first violated rule\n",
        "                # else:\n",
        "                    # Optional: Add a warning if a feature value is not numeric or is NaN\n",
        "                    # print(f\"Warning: Value for feature '{feat}' in row {i} is not numeric or is NaN.\")\n",
        "            # else:\n",
        "                # Optional: Add a warning if a feature from the rules is not in the new data\n",
        "                # print(f\"Warning: Feature '{feat}' from rules not found in row {i} or bounds are missing.\")\n",
        "\n",
        "        flags.append(is_anomaly) # Append the anomaly flag for the row\n",
        "    return flags\n",
        "\n",
        "# Test the rule-based detection on the original encoded dataframe\n",
        "# This serves as a check of how well the rules capture the detected anomalies\n",
        "print(\"\\nTesting rule-based detection on the dataset...\")\n",
        "if rules: # Only run if rules were generated\n",
        "    future_flags = detect_anomaly_with_rules(df_encoded, rules)\n",
        "    print(f\"Anomalies detected by rule engine: {sum(future_flags)} out of {len(df_encoded)} records.\")\n",
        "else:\n",
        "    print(\"Skipping rule-based detection test as no rules were generated.\")\n",
        "\n",
        "# Optional: Compare rule-based detection with model-based detection\n",
        "# if rules and shap_values is not None and shap_values.shape[0] > 0:\n",
        "#     print(f\"Anomalies detected by autoencoder model: {sum(anomalies)}\")\n",
        "#     # How many records flagged by rules were also flagged by the model? (True Positives if model is ground truth)\n",
        "#     true_positives = np.sum(np.logical_and(future_flags, anomalies))\n",
        "#     print(f\"Rule-based flags matching model anomalies: {true_positives}\")\n",
        "#     # How many records flagged by rules were *not* flagged by the model? (False Positives)\n",
        "#     false_positives = np.sum(np.logical_and(future_flags, ~anomalies))\n",
        "#     print(f\"Rule-based flags *not* matching model anomalies (False Positives): {false_positives}\")\n",
        "#     # How many records flagged by the model were *not* flagged by rules? (False Negatives)\n",
        "#     false_negatives = np.sum(np.logical_and(~future_flags, anomalies))\n",
        "#     print(f\"Model anomalies *not* caught by rules (False Negatives): {false_negatives}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "mgVY-VnQaoe5",
        "outputId": "fbd59640-d47b-495f-b894-64fb2b6f0d1c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training autoencoder...\n",
            "Error during autoencoder training: gradient registry has no entry for: shap_DivNoNan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "gradient registry has no entry for: shap_DivNoNan",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cdd834c67831>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Add error handling or print statements to confirm this step completes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Autoencoder training complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtrainable_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: gradient registry has no entry for: shap_DivNoNan"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure required libraries are installed\n",
        "# Note: We install shap and tensorflow together.\n",
        "# Sometimes specific versions work better; if this fails, try specifying versions.\n",
        "#!pip install --upgrade tensorflow shap pandas scikit-learn numpy matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector\n",
        "# Do NOT import shap or matplotlib.pyplot yet\n",
        "\n",
        "# 1. Load and Preprocess Data\n",
        "# Load the dataset generated in the previous cell\n",
        "df = pd.read_csv('synthetic_banking_transactions.csv')\n",
        "# Fill NaN values, although the synthetic data might not have them\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Convert categorical features to numerical using one-hot encoding\n",
        "# This is crucial for the autoencoder to process these features\n",
        "df_encoded = pd.get_dummies(df)\n",
        "\n",
        "# Normalize numerical features to be between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(df_encoded)\n",
        "\n",
        "# Reshape data for LSTM input: (samples, timesteps, features)\n",
        "# Here, each transaction is a single timestep\n",
        "X = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# 2. Build LSTM Autoencoder\n",
        "timesteps = X.shape[1] # Should be 1\n",
        "input_dim = X.shape[2] # Number of features after one-hot encoding and scaling\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "inputs = Input(shape=(timesteps, input_dim))\n",
        "# Encoder: LSTM layer to capture sequential patterns (even with timesteps=1)\n",
        "encoded = LSTM(64, activation='relu')(inputs)\n",
        "# Repeat the encoded vector for decoding\n",
        "decoded = RepeatVector(timesteps)(encoded)\n",
        "# Decoder: LSTM layer to reconstruct the input data\n",
        "decoded = LSTM(input_dim, activation='sigmoid', return_sequences=True)(decoded)\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Model(inputs, decoded)\n",
        "# Compile the model with Adam optimizer and mean squared error loss\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the autoencoder first\n",
        "# Use the scaled and reshaped data for training\n",
        "print(\"Training autoencoder...\")\n",
        "# The error occurs here. By not importing shap yet, we check if shap import/init is the cause.\n",
        "try:\n",
        "    autoencoder.fit(X, X, epochs=10, batch_size=32, validation_split=0.1, verbose=0)\n",
        "    print(\"Autoencoder training complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during autoencoder training *before* importing SHAP: {e}\")\n",
        "    # Re-raise the exception if it still occurs, but this time it's more definitive\n",
        "    # that the issue isn't caused by SHAP *initialization* after training.\n",
        "    raise\n",
        "\n",
        "# 3. Calculate Reconstruction Error\n",
        "# Predict the reconstructed output for the input data\n",
        "X_pred = autoencoder.predict(X)\n",
        "# Calculate the mean squared error between the original input and the reconstruction\n",
        "# Mean across timesteps (axis=1) and features (axis=2)\n",
        "recon_error = np.mean(np.power(X - X_pred, 2), axis=(1,2))\n",
        "# Define a threshold for anomalies (e.g., 95th percentile of reconstruction errors)\n",
        "threshold = np.percentile(recon_error, 95)\n",
        "# Identify anomalies where the reconstruction error exceeds the threshold\n",
        "anomalies = recon_error > threshold\n",
        "\n",
        "# Get the records from the original dataframe that are identified as anomalous\n",
        "anomaly_records = df[anomalies]\n",
        "print(f\"\\nIdentified {sum(anomalies)} anomalous records (using a threshold of {threshold:.4f}).\")\n",
        "print(\"Anomalous Records (first 5):\")\n",
        "# Import display only when needed\n",
        "from IPython.display import display\n",
        "display(anomaly_records.head()) # Use display for better formatting in notebooks\n",
        "\n",
        "# NOW, import shap and matplotlib.pyplot after the model is trained and anomalies are found\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 4. Use SHAP to Explain Anomalies using KernelExplainer\n",
        "# KernelExplainer requires a prediction function that takes 2D input\n",
        "# and a background dataset that is also 2D.\n",
        "# Our model expects 3D input (samples, timesteps, features).\n",
        "# Since timesteps=1, we can reshape the input for the prediction function.\n",
        "\n",
        "# Create a prediction function that reshapes 2D input to 3D for the model\n",
        "def predict_wrapper(x):\n",
        "    # Reshape the 2D input (samples, features) to 3D (samples, 1, features)\n",
        "    x_reshaped = np.reshape(x, (x.shape[0], 1, x.shape[1]))\n",
        "    # Predict using the autoencoder\n",
        "    prediction = autoencoder.predict(x_reshaped)\n",
        "    # Explain the mean squared error for each sample\n",
        "    return np.mean(np.power(x_reshaped - prediction, 2), axis=(1,2))\n",
        "\n",
        "# Flatten the scaled data for KernelExplainer background and explanation data\n",
        "X_flat = X.reshape(X.shape[0], -1) # Reshape (samples, 1, features) to (samples, features)\n",
        "\n",
        "# Select a background dataset for KernelExplainer (a sample of non-anomalous data)\n",
        "# KernelExplainer performance depends on the size and representativeness of the background data.\n",
        "# Let's sample from the non-anomalous data (anomalies == False)\n",
        "non_anomalies_indices = np.where(anomalies == False)[0]\n",
        "# Ensure there are non-anomalous points to sample from\n",
        "if len(non_anomalies_indices) > 100:\n",
        "    background_indices = np.random.choice(non_anomalies_indices, 100, replace=False)\n",
        "    background_data = X_flat[background_indices]\n",
        "    print(\"\\nInitializing SHAP KernelExplainer with 100 background samples...\")\n",
        "\n",
        "    # Initialize KernelExplainer with the prediction wrapper and background data\n",
        "    explainer = shap.KernelExplainer(predict_wrapper, background_data)\n",
        "\n",
        "    # Calculate SHAP values for the anomalous instances\n",
        "    # Use the flattened data for the anomalies\n",
        "    X_anomalies_flat = X_flat[anomalies]\n",
        "\n",
        "    # Check if there are anomalies to explain\n",
        "    if X_anomalies_flat.shape[0] > 0:\n",
        "        print(f\"Calculating SHAP values for {X_anomalies_flat.shape[0]} anomalous instances using KernelExplainer...\")\n",
        "        # KernelExplainer might print progress bars\n",
        "        # Limit the number of anomalies for explanation if there are too many, as KernelExplainer is slow\n",
        "        max_anomalies_to_explain = 50 # Explain at most 50 anomalies for speed\n",
        "        X_anomalies_flat_subset = X_anomalies_flat[:max_anomalies_to_explain]\n",
        "\n",
        "        # Pass feature names to the explainer init if possible or rely on the summary_plot's feature_names argument\n",
        "        # KernelExplainer can take feature_names in the constructor\n",
        "        explainer = shap.KernelExplainer(predict_wrapper, background_data, feature_names=df_encoded.columns.tolist())\n",
        "        shap_values = explainer.shap_values(X_anomalies_flat_subset)\n",
        "\n",
        "\n",
        "        # shap_values from KernelExplainer explaining a 1D output is typically a single array\n",
        "        # with shape (num_anomalies_subset, num_features).\n",
        "        # Display SHAP summary plot\n",
        "        print(\"\\nGenerating SHAP summary plot...\")\n",
        "        # shap.summary_plot expects SHAP values and the data used to compute them.\n",
        "        # Use the flattened anomaly data subset and feature names from df_encoded\n",
        "        shap.summary_plot(\n",
        "            shap_values, # SHAP values array (num_anomalies_subset, num_features)\n",
        "            X_anomalies_flat_subset, # Data points (2D: samples, features) corresponding to anomalies\n",
        "            feature_names=df_encoded.columns, # Feature names\n",
        "            plot_type='bar', # Use bar plot for overall feature importance\n",
        "            show=False # Prevent immediate plot display, control with plt.show()\n",
        "        )\n",
        "        plt.title(f'SHAP Feature Importance for High Reconstruction Error (Top {max_anomalies_to_explain} Anomalies)')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No anomalies found to explain with SHAP.\")\n",
        "        shap_values = None # Set to None if no anomalies\n",
        "\n",
        "else:\n",
        "    print(\"Not enough non-anomalous points to create a background dataset for SHAP.\")\n",
        "    shap_values = None # Set to None if no background data\n",
        "\n",
        "# 5. Derive Simple Rules from SHAP Values\n",
        "rules = {}\n",
        "# Proceed only if SHAP values were successfully calculated and are not None\n",
        "if shap_values is not None and shap_values.shape[0] > 0:\n",
        "    # shap_values shape is (num_anomalies_subset, num_features) for KernelExplainer explaining 1D output\n",
        "    # We want the mean absolute SHAP value for each feature across all explained anomalies\n",
        "    mean_shap = np.abs(shap_values).mean(axis=0) # Average across explained anomalies\n",
        "\n",
        "    # Get indices of top features based on mean absolute SHAP values\n",
        "    # Get the top 5 feature indices, ensuring we don't ask for more features than available\n",
        "    num_features_to_select = min(5, len(mean_shap))\n",
        "    top_features_indices = np.argsort(mean_shap)[-num_features_to_select:]\n",
        "\n",
        "    # Get the actual feature names using these indices\n",
        "    top_features = df_encoded.columns[top_features_indices].tolist()\n",
        "    print(f\"\\nTop {num_features_to_select} anomaly-driving features based on SHAP:\", top_features)\n",
        "\n",
        "    # Derive simple rules based on the distribution of these top features in the anomalous records\n",
        "    # Use the *full* set of encoded anomaly records (not just the subset used for SHAP explanation)\n",
        "    anomaly_records_for_rules = df_encoded[anomalies]\n",
        "\n",
        "    for feat in top_features:\n",
        "        # Check if the feature exists in the anomaly records DataFrame used for rules\n",
        "        if feat in anomaly_records_for_rules.columns:\n",
        "            values = anomaly_records_for_rules[feat]\n",
        "            # Calculate 5th and 95th percentiles as bounds for the rule\n",
        "            # Handle potential cases where the feature column is empty or all NaNs in anomalies\n",
        "            if not values.empty and not values.isnull().all():\n",
        "                 rules[feat] = {\n",
        "                    \"min\": values.quantile(0.05),\n",
        "                    \"max\": values.quantile(0.95)\n",
        "                }\n",
        "            else:\n",
        "                 print(f\"Warning: Feature '{feat}' in anomaly records is empty or contains only NaNs. Skipping rule generation for this feature.\")\n",
        "        else:\n",
        "            # This should ideally not happen if top_features come from df_encoded.columns\n",
        "            print(f\"Warning: Top feature '{feat}' not found in the anomaly records dataframe used for rules.\")\n",
        "\n",
        "    print(\"\\nGenerated rules (based on 5th/95th percentile of encoded anomalies for top SHAP features):\")\n",
        "    if rules:\n",
        "        for feat, bounds in rules.items():\n",
        "            if bounds: # Check if bounds were successfully generated\n",
        "                print(f\"If {feat} < {bounds['min']:.4f} or > {bounds['max']:.4f} → Possible anomaly\")\n",
        "    else:\n",
        "        print(\"No rules generated (either no anomalies or issue with feature processing).\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping rule generation as no SHAP values were calculated or anomalies found.\")\n",
        "\n",
        "\n",
        "# 6. Future Use: Rule-Based Filter\n",
        "# Define a function to detect anomalies using the derived rules\n",
        "def detect_anomaly_with_rules(new_data, rules):\n",
        "    \"\"\"\n",
        "    Applies the generated rules to new data to detect anomalies.\n",
        "\n",
        "    Args:\n",
        "        new_data (pd.DataFrame): The dataframe of new data (must contain the features\n",
        "                                 used in the rules, typically encoded/preprocessed\n",
        "                                 similarly to the data used for rule generation).\n",
        "        rules (dict): A dictionary containing features and their min/max bounds.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of boolean flags indicating if a record is an anomaly (True) or not (False).\n",
        "    \"\"\"\n",
        "    flags = []\n",
        "    # Iterate through each row of the new data\n",
        "    for i, row in new_data.iterrows():\n",
        "        is_anomaly = False\n",
        "        # Check each rule\n",
        "        for feat, bounds in rules.items():\n",
        "             # Ensure the feature exists in the current row and bounds were generated\n",
        "            if feat in row and bounds:\n",
        "                value = row[feat]\n",
        "                # Check if the value violates the defined bounds\n",
        "                # Ensure value is numeric before comparison\n",
        "                if pd.api.types.is_numeric_dtype(type(value)) and not np.isnan(value):\n",
        "                     if value < bounds[\"min\"] or value > bounds[\"max\"]:\n",
        "                        is_anomaly = True # Flag as anomaly if any rule is violated\n",
        "                        break # Optional: Uncomment to flag based on the first violated rule\n",
        "                # else:\n",
        "                    # Optional: Add a warning if a feature value is not numeric or is NaN\n",
        "                    # print(f\"Warning: Value for feature '{feat}' in row {i} is not numeric or is NaN.\")\n",
        "            # else:\n",
        "                # Optional: Add a warning if a feature from the rules is not in the new data\n",
        "                # print(f\"Warning: Feature '{feat}' from rules not found in row {i} or bounds are missing.\")\n",
        "\n",
        "        flags.append(is_anomaly) # Append the anomaly flag for the row\n",
        "    return flags\n",
        "\n",
        "# Test the rule-based detection on the original encoded dataframe\n",
        "# This serves as a check of how well the rules capture the detected anomalies\n",
        "print(\"\\nTesting rule-based detection on the dataset...\")\n",
        "if rules: # Only run if rules were generated\n",
        "    future_flags = detect_anomaly_with_rules(df_encoded, rules)\n",
        "    print(f\"Anomalies detected by rule engine: {sum(future_flags)} out of {len(df_encoded)} records.\")\n",
        "else:\n",
        "    print(\"Skipping rule-based detection test as no rules were generated.\")\n",
        "\n",
        "# Optional: Compare rule-based detection with model-based detection\n",
        "# if rules and shap_values is not None and shap_values.shape[0] > 0:\n",
        "#     print(f\"Anomalies detected by autoencoder model: {sum(anomalies)}\")\n",
        "#     # How many records flagged by rules were also flagged by the model? (True Positives if model is ground truth)\n",
        "#     true_positives = np.sum(np.logical_and(future_flags, anomalies))\n",
        "#     print(f\"Rule-based flags matching model anomalies: {true_positives}\")\n",
        "#     # How many records flagged by rules were *not* flagged by the model? (False Positives)\n",
        "#     false_positives = np.sum(np.logical_and(future_flags, ~anomalies))\n",
        "#     print(f\"Rule-based flags *not* matching model anomalies (False Positives): {false_positives}\")\n",
        "#     # How many records flagged by the model were *not* flagged by rules? (False Negatives)\n",
        "#     false_negatives = np.sum(np.logical_and(~future_flags, anomalies))\n",
        "#     print(f\"Model anomalies *not* caught by rules (False Negatives): {false_negatives}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770,
          "referenced_widgets": [
            "7f23b4fe4cec4e058187f3871d8355b7",
            "d83f31c58f554c32bc7d73f8876584d4",
            "2319bf23767242bebf5ab8e48b67808c",
            "ddde5ea543fd4e76bdb7beb550cd6515",
            "553b6dd48ba44a45a60793cb851e8962",
            "f53b2bb6d8fa41e097bb63263985a06f",
            "a0bc90ccd21f4094a46d0c84fbf85374",
            "0181352df4a34ec7a2e0151d0adb1d11",
            "b23bd7b2bfb2467da45ced9786263a7f",
            "9ef9aea76e52400a8a500119cea99626",
            "2531d3d9a4e54ceba090d895eccffe01"
          ]
        },
        "id": "v9E8lzEIbqft",
        "outputId": "c0c5cbb4-2901-43db-901b-dde467016723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training autoencoder...\n",
            "Autoencoder training complete.\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step\n",
            "\n",
            "Identified 53 anomalous records (using a threshold of 0.0040).\n",
            "Anomalous Records (first 5):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     Customer ID       Customer Name  \\\n",
              "3              8      Kristi Vasquez   \n",
              "78            16       Shelby Curtis   \n",
              "86           181      Patrick Weaver   \n",
              "140          113      Anthony Arnold   \n",
              "146          139  Brandon Turner PhD   \n",
              "\n",
              "                                               Address  Country  \\\n",
              "3    920 Ortiz Key Suite 840, Katherineburgh, AL 12168   Canada   \n",
              "78   44289 Jenkins Trail Apt. 926, Jordanville, IL ...   Canada   \n",
              "86    9431 Ford Meadow Apt. 061, East Amanda, IN 07544  Germany   \n",
              "140                   PSC 4233, Box 3656, APO AE 35739   Canada   \n",
              "146        90804 Craig Skyway, Williamsburgh, AL 85558   Canada   \n",
              "\n",
              "          Transaction Timestamp  Transaction Amount Transaction Location  \\\n",
              "3    2025-02-14 09:15:04.434398             1263.96               Canada   \n",
              "78   2024-07-06 09:30:04.415946             3185.98               Canada   \n",
              "86   2025-04-16 08:24:04.451214            95301.50              Unknown   \n",
              "140  2024-08-19 15:42:04.298986             2609.44               Canada   \n",
              "146  2024-08-27 01:57:04.077709             3522.22               Canada   \n",
              "\n",
              "     Payer Customer ID  Payee Customer ID Risk Profile  Account Number  \\\n",
              "3                    8                542          Low       100000008   \n",
              "78                  16                128         High       100000016   \n",
              "86                 181                 31         High       100000181   \n",
              "140                113                 39          Low       100000113   \n",
              "146                139                 78       Medium       100000139   \n",
              "\n",
              "      Account Type Channel Account Status  Base Balance Currency  \n",
              "3          Savings     ATM         Active      86751.44      INR  \n",
              "78         Savings     ATM         Closed      19157.05      INR  \n",
              "86   Fixed Deposit  Online         Closed      34765.57      USD  \n",
              "140        Savings     ATM         Active      93040.07      EUR  \n",
              "146        Current     ATM       Inactive      36999.33      EUR  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2c0007f-d628-422a-8048-5576083ade65\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>Address</th>\n",
              "      <th>Country</th>\n",
              "      <th>Transaction Timestamp</th>\n",
              "      <th>Transaction Amount</th>\n",
              "      <th>Transaction Location</th>\n",
              "      <th>Payer Customer ID</th>\n",
              "      <th>Payee Customer ID</th>\n",
              "      <th>Risk Profile</th>\n",
              "      <th>Account Number</th>\n",
              "      <th>Account Type</th>\n",
              "      <th>Channel</th>\n",
              "      <th>Account Status</th>\n",
              "      <th>Base Balance</th>\n",
              "      <th>Currency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>Kristi Vasquez</td>\n",
              "      <td>920 Ortiz Key Suite 840, Katherineburgh, AL 12168</td>\n",
              "      <td>Canada</td>\n",
              "      <td>2025-02-14 09:15:04.434398</td>\n",
              "      <td>1263.96</td>\n",
              "      <td>Canada</td>\n",
              "      <td>8</td>\n",
              "      <td>542</td>\n",
              "      <td>Low</td>\n",
              "      <td>100000008</td>\n",
              "      <td>Savings</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Active</td>\n",
              "      <td>86751.44</td>\n",
              "      <td>INR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>16</td>\n",
              "      <td>Shelby Curtis</td>\n",
              "      <td>44289 Jenkins Trail Apt. 926, Jordanville, IL ...</td>\n",
              "      <td>Canada</td>\n",
              "      <td>2024-07-06 09:30:04.415946</td>\n",
              "      <td>3185.98</td>\n",
              "      <td>Canada</td>\n",
              "      <td>16</td>\n",
              "      <td>128</td>\n",
              "      <td>High</td>\n",
              "      <td>100000016</td>\n",
              "      <td>Savings</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Closed</td>\n",
              "      <td>19157.05</td>\n",
              "      <td>INR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>181</td>\n",
              "      <td>Patrick Weaver</td>\n",
              "      <td>9431 Ford Meadow Apt. 061, East Amanda, IN 07544</td>\n",
              "      <td>Germany</td>\n",
              "      <td>2025-04-16 08:24:04.451214</td>\n",
              "      <td>95301.50</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>181</td>\n",
              "      <td>31</td>\n",
              "      <td>High</td>\n",
              "      <td>100000181</td>\n",
              "      <td>Fixed Deposit</td>\n",
              "      <td>Online</td>\n",
              "      <td>Closed</td>\n",
              "      <td>34765.57</td>\n",
              "      <td>USD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>113</td>\n",
              "      <td>Anthony Arnold</td>\n",
              "      <td>PSC 4233, Box 3656, APO AE 35739</td>\n",
              "      <td>Canada</td>\n",
              "      <td>2024-08-19 15:42:04.298986</td>\n",
              "      <td>2609.44</td>\n",
              "      <td>Canada</td>\n",
              "      <td>113</td>\n",
              "      <td>39</td>\n",
              "      <td>Low</td>\n",
              "      <td>100000113</td>\n",
              "      <td>Savings</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Active</td>\n",
              "      <td>93040.07</td>\n",
              "      <td>EUR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>139</td>\n",
              "      <td>Brandon Turner PhD</td>\n",
              "      <td>90804 Craig Skyway, Williamsburgh, AL 85558</td>\n",
              "      <td>Canada</td>\n",
              "      <td>2024-08-27 01:57:04.077709</td>\n",
              "      <td>3522.22</td>\n",
              "      <td>Canada</td>\n",
              "      <td>139</td>\n",
              "      <td>78</td>\n",
              "      <td>Medium</td>\n",
              "      <td>100000139</td>\n",
              "      <td>Current</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Inactive</td>\n",
              "      <td>36999.33</td>\n",
              "      <td>EUR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2c0007f-d628-422a-8048-5576083ade65')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b2c0007f-d628-422a-8048-5576083ade65 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b2c0007f-d628-422a-8048-5576083ade65');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8bd0b22f-720d-476e-946c-104107833a55\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8bd0b22f-720d-476e-946c-104107833a55')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8bd0b22f-720d-476e-946c-104107833a55 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing SHAP KernelExplainer with 100 background samples...\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "Calculating SHAP values for 53 anomalous instances using KernelExplainer...\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f23b4fe4cec4e058187f3871d8355b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NlIVqLEIXThy"
      }
    }
  ]
}